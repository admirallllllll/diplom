{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "!pip install node2vec --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitrina = pd.read_excel('/Users/diananigmatullina/Downloads/vitrina_clusters.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "window = vitrina\n",
    "\n",
    "# Add nodes (customers) to the graph\n",
    "for idx, row in window.iterrows():\n",
    "    customer_id = idx\n",
    "    G.add_node(customer_id)  # Add node without cluster attribute\n",
    "    G.nodes[customer_id]['cluster'] = row['cluster'] \n",
    "\n",
    "\n",
    "\n",
    "n_neighbors = 10\n",
    "\n",
    "# Loop through each unique cluster ID\n",
    "for cluster_id in window['cluster'].unique():\n",
    "    # Get the indices of customers in the current cluster\n",
    "    cluster_customers = window[window['cluster'] == cluster_id].index\n",
    "    \n",
    "    # Ensure there are enough customers in the cluster\n",
    "    if len(cluster_customers) >= n_neighbors:\n",
    "        # Extract features\n",
    "        cluster_features = window.loc[cluster_customers, ['Number of bills', 'Average bill', \n",
    "                                                          'Average number of goods in bill', \n",
    "                                                          'Revenue', 'Discount']]\n",
    "        \n",
    "        # Initialize NearestNeighbors model\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "        nbrs.fit(cluster_features)\n",
    "        \n",
    "        # Find nearest neighbors for each customer\n",
    "        distances, indices = nbrs.kneighbors(cluster_features)\n",
    "        \n",
    "        # Add edges between nearest neighbors\n",
    "        for i, customer_index in enumerate(cluster_customers):\n",
    "            for neighbor_index in indices[i]:\n",
    "                if customer_index != cluster_customers[neighbor_index]:\n",
    "                    G.add_edge(customer_index, cluster_customers[neighbor_index])\n",
    "    else:\n",
    "        print(f\"Not enough customers in cluster {cluster_id} to find {n_neighbors} neighbors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "# Sample a subset of nodes and edges\n",
    "sampled_nodes = random.sample(G.nodes(), k=1000)\n",
    "sampled_edges = random.sample(G.edges(), k=2000)\n",
    "\n",
    "# Create a new graph with the sampled nodes and edges\n",
    "subgraph = nx.Graph()\n",
    "subgraph.add_nodes_from(sampled_nodes)\n",
    "subgraph.add_edges_from(sampled_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec = Node2Vec(subgraph, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings = {}\n",
    "for node in subgraph.nodes():\n",
    "    if node in model.wv:\n",
    "        node_embeddings[node] = model.wv[node]\n",
    "    else:\n",
    "        print(f\"Node {node} is not present in the embeddings model.\")\n",
    "\n",
    "# Use only the embeddings for nodes present in the model\n",
    "node_embeddings = {node: model.wv[node] for node in node_embeddings.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use labels from the original window dataframe\n",
    "labels = window['cluster']\n",
    "\n",
    "# Filter node embeddings and labels to include only nodes with labels\n",
    "valid_nodes = set(labels.index)\n",
    "filtered_node_embeddings = {node: embedding for node, embedding in node_embeddings.items() if node in valid_nodes}\n",
    "filtered_labels = labels.loc[list(filtered_node_embeddings.keys())]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(filtered_node_embeddings.values()), filtered_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train a classifier (e.g., logistic regression) on the node embeddings\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Recommendations (Revised)\n",
    "def generate_recommendations(model, user_embeddings, items, k=10):\n",
    "    # Predict scores for items\n",
    "    item_scores = model.predict_proba(user_embeddings)[:, 1]  # Assuming positive class index is 1\n",
    "    \n",
    "    # Sort items based on scores\n",
    "    ranked_items = sorted(zip(items, item_scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top-k recommendations\n",
    "    top_recommendations = ranked_items[:k]\n",
    "    \n",
    "    return top_recommendations\n",
    "\n",
    "\n",
    "# Initialize lists to store recommendations and ground truth\n",
    "all_recommendations = []\n",
    "all_ground_truth = []\n",
    "\n",
    "# Iterate over each user in the test set\n",
    "for user_index in range(len(X_test)):\n",
    "    # Generate recommendations for the current user\n",
    "    user_embedding = X_test[user_index].reshape(1, -1)\n",
    "    recommendations = generate_recommendations(classifier, user_embedding, list(range(num_items)), k=10)\n",
    "    \n",
    "    # Append recommendations to the list\n",
    "    all_recommendations.append([item for item, _ in recommendations])\n",
    "    \n",
    "    # Get ground truth for the current user\n",
    "    ground_truth_labels = window.loc[y_test.index[user_index], window.columns[7:]]  # Assuming item columns start from index 7\n",
    "    ground_truth = [item for item, count in ground_truth_labels.items() if count > 0]\n",
    "    \n",
    "    # Append ground truth to the list\n",
    "    all_ground_truth.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "# Calculate Mean Average Precision (MAP)\n",
    "average_precisions = []\n",
    "for ground_truth, recommendations in zip(all_ground_truth, all_recommendations):\n",
    "    y_true = [1 if item in ground_truth else 0 for item in recommendations]\n",
    "    y_score = [1 if item in ground_truth else 0 for item in recommendations]\n",
    "    average_precisions.append(average_precision_score(y_true, y_score))\n",
    "mean_average_precision = np.mean(average_precisions)\n",
    "\n",
    "# Print or use the calculated metrics as needed\n",
    "print(\"Mean Average Precision (MAP):\", mean_average_precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import random\n",
    "\n",
    "def random_walk(graph, node, walk_length):\n",
    "    walk = [node]\n",
    "    for _ in range(walk_length - 1):\n",
    "        neighbors = list(graph.neighbors(walk[-1]))\n",
    "        if neighbors:\n",
    "            walk.append(random.choice(neighbors))\n",
    "        else:\n",
    "            break\n",
    "    return walk\n",
    "\n",
    "walks = []\n",
    "num_walks = 10\n",
    "walk_length = 80\n",
    "G = subgraph\n",
    "for node in G.nodes():\n",
    "    for _ in range(num_walks):\n",
    "        walks.append(random_walk(G, node, walk_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model\n",
    "embedding_size = 100\n",
    "window_size = 10\n",
    "model = Word2Vec(walks, vector_size=embedding_size, window=window_size, min_count=0, sg=1, workers=4)\n",
    "\n",
    "# Get node embeddings\n",
    "node_embeddings = {node: model.wv[node] for node in G.nodes()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Extract features and labels\n",
    "features = ['Number of bills', 'Average bill', 'Average number of goods in bill', 'Revenue', 'Discount']\n",
    "X = window[features]\n",
    "\n",
    "# Convert columns representing certain items into binary labels\n",
    "items_columns = [\n",
    "    '%TN_Автотовары', '%TN_Аксессуары', '%TN_Детские товары', '%TN_Игры, софт и развлечения',\n",
    "    '%TN_Хобби, досуг', '%TN_Цифровая Техника', '%TN_Элитная техника'\n",
    "]\n",
    "y = window[items_columns]\n",
    "y = (y > 0).any(axis=1).astype(int)  \n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict whether a person will buy a certain item\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already defined and trained your random forest classifier (rf_classifier) and generated node embeddings and node order\n",
    "\n",
    "def make_predictions_per_customer(node_embeddings, rf_classifier, node_order):\n",
    "    predictions_per_customer = {}\n",
    "    for node in node_order:\n",
    "        if node in node_embeddings:\n",
    "            # Retrieve embedding for the current node\n",
    "            embedding = node_embeddings[node]\n",
    "            # Ensure the embedding has the correct shape\n",
    "            embedding = np.reshape(embedding, (1, -1))  # Reshape to match the expected shape\n",
    "            # Use the trained random forest classifier to make predictions for this customer\n",
    "            predictions = rf_classifier.predict_proba(embedding)\n",
    "            predictions_per_customer[node] = predictions\n",
    "        else:\n",
    "            print(f\"Warning: No embedding found for node {node}\")\n",
    "            predictions_per_customer[node] = None\n",
    "    return predictions_per_customer\n",
    "\n",
    "# Step 4: Generate Recommendations\n",
    "def generate_recommendations(predictions_per_customer, item_names, top_n=5):\n",
    "    recommendations_per_customer = {}\n",
    "    for node, predictions in predictions_per_customer.items():\n",
    "        if predictions is not None:\n",
    "            # Sort predicted probabilities in descending order and get indices\n",
    "            top_indices = np.argsort(predictions[0])[::-1][:top_n]\n",
    "            # Get the top recommended items\n",
    "            top_recommendations = [(item_names[i], predictions[0][i]) for i in top_indices]\n",
    "            recommendations_per_customer[node] = top_recommendations\n",
    "        else:\n",
    "            recommendations_per_customer[node] = None\n",
    "    return recommendations_per_customer\n",
    "\n",
    "node_order = sorted(subgraph.nodes())\n",
    "\n",
    "predictions_per_customer = make_predictions_per_customer(node_embeddings, rf_classifier, node_order)\n",
    "\n",
    "recommendations_per_customer = generate_recommendations(predictions_per_customer, item_names, top_n=5)\n",
    "\n",
    "# Print recommendations for each customer\n",
    "for node, recommendations in recommendations_per_customer.items():\n",
    "    if recommendations is not None:\n",
    "        print(f\"Recommendations for customer {node}:\")\n",
    "        for rank, (item, probability) in enumerate(recommendations):\n",
    "            print(f\"{rank+1}. {item} (Probability: {probability:.4f})\")\n",
    "    else:\n",
    "        print(f\"No recommendations available for customer {node}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_scores = []\n",
    "recommended_items = []\n",
    "\n",
    "# Define the number of recommendations to generate for each customer\n",
    "num_recommendations = 3  \n",
    "\n",
    "# Iterate over each customer\n",
    "for customer_id in subgraph.nodes():\n",
    "    # Generate recommendations for the customer\n",
    "    recommendations = generate_recommendations(customer_id, num_recommendations)\n",
    "    \n",
    "    # Extract the ground truth (items actually purchased by the customer)\n",
    "    ground_truth = window[window['Phone_new'] == customer_id][items_columns].values.flatten().tolist()\n",
    "    \n",
    "    # Prepare relevance scores for the customer's purchased items\n",
    "    relevance = [1 if item in ground_truth else 0 for item in recommended_items]\n",
    "    relevance_scores.append(relevance)\n",
    "    \n",
    "    # Extract recommended items\n",
    "    recommended_items.append([item[0] for item in recommendations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Mean Average Precision (MAP)\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "maps = []\n",
    "for relevance in relevance_scores:\n",
    "    if relevance:\n",
    "        map_score = average_precision_score(relevance, range(1, len(relevance) + 1))\n",
    "        maps.append(map_score)\n",
    "    else:\n",
    "        maps.append(0)\n",
    "\n",
    "mean_map = sum(maps) / len(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Average Precision (MAP):\", mean_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Normalized Discounted Cumulative Gain (NDCG)\n",
    "ndcgs = []\n",
    "for relevance in relevance_scores:\n",
    "    if relevance:\n",
    "        ndcg_score = ndcg_score([relevance], [range(1, len(relevance) + 1)])\n",
    "        ndcgs.append(ndcg_score)\n",
    "    else:\n",
    "        # If relevance list is empty, append 0\n",
    "        ndcgs.append(0)\n",
    "\n",
    "mean_ndcg = sum(ndcgs) / len(ndcgs)\n",
    "print(\"Mean Normalized Discounted Cumulative Gain (NDCG):\", mean_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
